{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "#cur_path = \"/content/drive/MyDrive/SNA/final_project/\"\n",
        "cur_path = \"/content/drive/MyDrive/Colab Notebooks/final-project/\"\n",
        "os.chdir(cur_path)\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHMmq8SfFChC",
        "outputId": "ee7bbb75-ab11-4fb1-cd13-e6e5d75ccad0"
      },
      "id": "ZHMmq8SfFChC",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/final-project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "73c8dcf0",
      "metadata": {
        "id": "73c8dcf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a255f88-3ce4-4a49-db16-8eeaa8daf8d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.9 MB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 7.1 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "# # Environment Setup (note that capture silences the console output)\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; print(torch.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HttWS_9B1xs",
        "outputId": "75b38bd4-50fc-4d2b-cf9a-fa0c236a5161"
      },
      "id": "0HttWS_9B1xs",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "!pip install torch-geometric\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.11.0+cu113.htmlve\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "!pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 torchaudio==0.10.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxkQEkmpB832",
        "outputId": "82a4857c-3951-4895-e51c-9fdefb3e8c6d"
      },
      "id": "nxkQEkmpB832",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.13)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.htmlve\n",
            "Collecting torch-cluster\n",
            "  Downloading torch_cluster-1.6.0.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 215 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: torch-cluster\n",
            "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-cluster: filename=torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl size=309643 sha256=2c85bd8be5284db75c1119afcf6d332c2a04d6eb03500fbd9bcdf04a61873b30\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/c7/3e/258dd72b35d7a459264143ad5bfe97b9dc5eef90069ca2f13f\n",
            "Successfully built torch-cluster\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n",
            "\u001b[K     |████████████████████████████████| 750 kB 6.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2137.6 MB)\n",
            "\u001b[K     |████████████▌                   | 834.1 MB 1.3 MB/s eta 0:16:22tcmalloc: large alloc 1147494400 bytes == 0x391b8000 @  0x7fbbe96ad615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |███████████████▉                | 1055.7 MB 1.3 MB/s eta 0:14:01tcmalloc: large alloc 1434370048 bytes == 0x7d80e000 @  0x7fbbe96ad615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████            | 1336.2 MB 1.7 MB/s eta 0:07:49tcmalloc: large alloc 1792966656 bytes == 0x2640000 @  0x7fbbe96ad615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |█████████████████████████▎      | 1691.1 MB 1.6 MB/s eta 0:04:45tcmalloc: large alloc 2241208320 bytes == 0x6d428000 @  0x7fbbe96ad615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 2137.6 MB 1.3 MB/s eta 0:00:01tcmalloc: large alloc 2137645056 bytes == 0xf2d8a000 @  0x7fbbe96ac1e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9\n",
            "tcmalloc: large alloc 2672058368 bytes == 0x1e68ac000 @  0x7fbbe96ad615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n",
            "\u001b[K     |████████████████████████████████| 2137.6 MB 350 bytes/s \n",
            "\u001b[?25hCollecting torchvision==0.11.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.11.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (21.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.9 MB 78.7 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.10.0\n",
            "  Downloading https://download.pytorch.org/whl/rocm4.1/torchaudio-0.10.0%2Brocm4.1-cp37-cp37m-linux_x86_64.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 37.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0+cu111) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.0+cu111) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.0+cu111) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.11.0+cu113\n",
            "    Uninstalling torchaudio-0.11.0+cu113:\n",
            "      Successfully uninstalled torchaudio-0.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.0+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.0+cu111 torchaudio-0.10.0+rocm4.1 torchvision-0.11.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch_geometric\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import degree\n",
        "from sklearn import preprocessing as pp\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ujn3n6ykEogc"
      },
      "id": "ujn3n6ykEogc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "TmYynXocE71Q"
      },
      "id": "TmYynXocE71Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df = pd.read_csv(\"transactions_train.csv.zip\", nrows=20000)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "WA2OzguTE_6D"
      },
      "id": "WA2OzguTE_6D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split train and test data\n",
        "train, test = train_test_split(df.values, test_size=0.3, random_state=1)\n",
        "train_df = pd.DataFrame(train, columns=df.columns)\n",
        "test_df = pd.DataFrame(test, columns=df.columns)"
      ],
      "metadata": {
        "id": "bvWTDw0jFhDp"
      },
      "id": "bvWTDw0jFhDp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df # len"
      ],
      "metadata": {
        "id": "XBJGSoGovUhw"
      },
      "id": "XBJGSoGovUhw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df # len"
      ],
      "metadata": {
        "id": "eMdQ3Z_RvWwv"
      },
      "id": "eMdQ3Z_RvWwv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode IDs to be numeric\n",
        "customer_codes = pp.LabelEncoder()\n",
        "article_codes = pp.LabelEncoder()\n",
        "train_df['customer_id_idx'] = customer_codes.fit_transform(train_df['customer_id'].values)\n",
        "train_df['article_id_idx'] = article_codes.fit_transform(train_df['article_id'].values)"
      ],
      "metadata": {
        "id": "6pyihny7GYrg"
      },
      "id": "6pyihny7GYrg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of unique customers in training set\n",
        "train_customer_ids = train_df['customer_id'].unique()\n",
        "len(train_customer_ids)"
      ],
      "metadata": {
        "id": "4actGLFxFtpJ"
      },
      "id": "4actGLFxFtpJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of unique articles in training set\n",
        "train_article_ids = train_df['article_id'].unique()\n",
        "len(train_article_ids)"
      ],
      "metadata": {
        "id": "seYBjtiL4yrK"
      },
      "id": "seYBjtiL4yrK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_df))\n",
        "# filter test set to only include IDs in the training set\n",
        "test_df = test_df[(test_df['customer_id'].isin(train_customer_ids)) & (test_df['article_id'].isin(train_article_ids))]\n",
        "len(test_df)"
      ],
      "metadata": {
        "id": "GTnC8hRu4uuk"
      },
      "id": "GTnC8hRu4uuk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# redo label encoder\n",
        "test_df['customer_id_idx'] = customer_codes.transform(test_df['customer_id'].values)\n",
        "test_df['article_id_idx'] = article_codes.transform(test_df['article_id'].values)"
      ],
      "metadata": {
        "id": "wZFCyEoiGKAO"
      },
      "id": "wZFCyEoiGKAO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_customers = train_df['customer_id_idx'].nunique()\n",
        "n_customers"
      ],
      "metadata": {
        "id": "vUctyxsmGm1X"
      },
      "id": "vUctyxsmGm1X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_articles = train_df['article_id_idx'].nunique()\n",
        "n_articles"
      ],
      "metadata": {
        "id": "obK4IW_h5CoA"
      },
      "id": "obK4IW_h5CoA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_loader(data, batch_size, n_cust, n_art):\n",
        "\n",
        "    def samp_neg(x): # randomly samples if a product was not purchased by a customer\n",
        "        while True:\n",
        "            neg_id = random.randint(0, n_art - 1)\n",
        "            if neg_id not in x:\n",
        "                return neg_id\n",
        "\n",
        "    int_items_df = data.groupby('customer_id_idx')['article_id_idx'].apply(list).reset_index()\n",
        "    indices = [x for x in range(n_cust)] # create indicies for number of customers\n",
        "\n",
        "    if n_cust < batch_size: # sample for batch size\n",
        "        customers = [random.choice(indices) for _ in range(batch_size)] # random sample customer\n",
        "    else:\n",
        "        customers = random.sample(indices, batch_size)\n",
        "    customers.sort()\n",
        "    customers_df = pd.DataFrame(customers,columns = ['customers'])\n",
        "\n",
        "    int_items_df = pd.merge(int_items_df, customers_df, how = 'right', left_on = 'customer_id_idx', right_on = 'customers')\n",
        "    pos_items = int_items_df['article_id_idx'].apply(lambda x : random.choice(x)).values # sample if a customer bought an article\n",
        "    neg_items = int_items_df['article_id_idx'].apply(lambda x: samp_neg(x)).values # sample if a customer did not buy an article\n",
        "\n",
        "    return (\n",
        "        torch.LongTensor(list(customers)).to(device), \n",
        "        torch.LongTensor(list(pos_items)).to(device) + n_cust, \n",
        "        torch.LongTensor(list(neg_items)).to(device) + n_cust\n",
        "    )"
      ],
      "metadata": {
        "id": "z6L-Pm6XG7UN"
      },
      "id": "z6L-Pm6XG7UN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cust_t = torch.LongTensor(train_df.customer_id_idx) # customer tensor\n",
        "art_t = torch.LongTensor(train_df.article_id_idx) + n_customers # article tensor \n",
        "\n",
        "train_edge_index = torch.stack(( \n",
        "  torch.cat([cust_t, art_t]),\n",
        "  torch.cat([art_t, cust_t])\n",
        ")).to(device)\n",
        "train_edge_index # edge index list and reversed"
      ],
      "metadata": {
        "id": "8YLcsXpzH2jE"
      },
      "id": "8YLcsXpzH2jE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(customers_emb, pos_emb, neg_emb):\n",
        "  pos_scores = torch.mul(customers_emb, pos_emb).sum(dim=1) # bought article\n",
        "  neg_scores = torch.mul(customers_emb, neg_emb).sum(dim=1) # did not buy article\n",
        "      \n",
        "  loss = torch.mean(F.softplus(neg_scores - pos_scores,beta=1.5)) # softplus loss, beta =1.5\n",
        "      \n",
        "  return loss"
      ],
      "metadata": {
        "id": "yqiRq9bcI3cD"
      },
      "id": "yqiRq9bcI3cD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RecModel(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      latent_dim, \n",
        "      num_layers,\n",
        "      num_customers,\n",
        "      num_articles,\n",
        "      #model, # 'NGCF' or 'LightGCN'\n",
        "      dropout=0.1 # Only used in NGCF\n",
        "  ):\n",
        "    class NNColabFilter(MessagePassing):\n",
        "        def __init__(self, latent_dim, dropout, bias=True, **kwargs):  \n",
        "          super(NNColabFilter, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "          self.dropout = dropout\n",
        "\n",
        "          self.lin_1 = nn.Linear(latent_dim, latent_dim, bias=bias) # layer 1\n",
        "          self.lin_2 = nn.Linear(latent_dim, latent_dim, bias=bias) # layer 2\n",
        "\n",
        "          self.init_parameters() # initialize params\n",
        "\n",
        "\n",
        "        def init_parameters(self): # ramdom sample params\n",
        "          nn.init.xavier_uniform_(self.lin_1.weight) \n",
        "          nn.init.xavier_uniform_(self.lin_2.weight)\n",
        "\n",
        "\n",
        "        def forward(self, x, edge_index):\n",
        "          # normalize\n",
        "          from_, to_ = edge_index\n",
        "          deg = degree(to_, x.size(0), dtype=x.dtype)\n",
        "          deg_inv_sqrt = deg.pow(-0.5)\n",
        "          deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "          norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n",
        "\n",
        "          # messages\n",
        "          out = self.propagate(edge_index, x=(x, x), norm=norm)\n",
        "\n",
        "          # aggregate\n",
        "          out += self.lin_1(x)\n",
        "\n",
        "          # update \n",
        "          out = F.dropout(out, self.dropout, self.training)\n",
        "          return F.softmax(out)\n",
        "\n",
        "        def message(self, x_j, x_i, norm): # messages\n",
        "          return norm.view(-1, 1) * (self.lin_1(x_j) + self.lin_2(x_j * x_i))\n",
        "    \n",
        "    super(RecModel, self).__init__()\n",
        "    self.model = NNColabFilter(latent_dim, dropout) # establish model\n",
        "    self.embedding = nn.Embedding(num_customers + num_articles, latent_dim) # establish input embeddings\n",
        "\n",
        "\n",
        "    self.convs = nn.ModuleList(NNColabFilter(latent_dim, dropout=dropout) for _ in range(num_layers))\n",
        "\n",
        "    self.model.init_parameters() # initialize params\n",
        "\n",
        "\n",
        "\n",
        "  def init_parameters(self): # random sample params\n",
        "    nn.init.xavier_uniform_(self.embedding.weight, gain=1)\n",
        "    \n",
        "  def forward(self, edge_index):\n",
        "    # message\n",
        "    emb0 = self.embedding.weight\n",
        "    embs = [emb0]\n",
        "\n",
        "    # aggregate\n",
        "    emb = emb0\n",
        "    for conv in self.convs:\n",
        "      emb = conv(x=emb, edge_index=edge_index)\n",
        "      embs.append(emb)\n",
        "\n",
        "    # update\n",
        "    out = (\n",
        "      torch.cat(embs, dim=-1) )\n",
        "    \n",
        "    return emb0, out\n",
        "\n",
        "\n",
        "  def encode_minibatch(self, customers, pos_items, neg_items, edge_index):\n",
        "    emb0, out = self(edge_index)\n",
        "    return (\n",
        "        out[customers], \n",
        "        out[pos_items], \n",
        "        out[neg_items], \n",
        "        emb0[customers],\n",
        "        emb0[pos_items],\n",
        "        emb0[neg_items]\n",
        "    )"
      ],
      "metadata": {
        "id": "tB__6dnzg22e"
      },
      "id": "tB__6dnzg22e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_preds_and_score(customer_Embed_wts, article_Embed_wts, n_customers, n_articles, train_df, test_df, K=12):\n",
        "  test_customer_ids = torch.LongTensor(train_df.customer_id_idx.unique())\n",
        "  # relation score\n",
        "  rel_score = torch.mm(customer_Embed_wts, torch.transpose(article_Embed_wts, 0, 1))\n",
        "  # purchase interaction between customers and articles\n",
        "  stacked_interation = torch.stack((torch.LongTensor(train_df.customer_id_idx.values),torch.LongTensor(train_df.article_id_idx.values)))\n",
        "  # create purchas matrix\n",
        "  interaction_vector = torch.ones((len(train_df)), dtype=torch.float64)\n",
        "  # establish fully connected \n",
        "  interaction_tensor = torch.sparse.FloatTensor(stacked_interation, interaction_vector, (n_customers, n_articles)).to_dense().to(device)\n",
        "  # new relation score\n",
        "  final_rel_score = torch.mul(rel_score, (1-interaction_tensor))\n",
        "  # most related nodes\n",
        "  top_rel_indices = torch.topk(final_rel_score, K).indices\n",
        "  # create reltion df\n",
        "  top_rel_indices_df = pd.DataFrame(top_rel_indices.cpu().numpy(), columns = ['Top_Article_Index_'+str(x+1) for x in range(0,K)])\n",
        "  top_rel_indices_df['customer_id'] = top_rel_indices_df.index\n",
        "  # related article predictions\n",
        "  top_rel_indices_df['top_rel_articles_preds'] = top_rel_indices_df[['Top_Article_Index_'+str(x+1) for x in range(0,K)]].values.tolist()\n",
        "  top_rel_indices_df = top_rel_indices_df[['customer_id', 'top_rel_articles_preds']]\n",
        "\n",
        "  ##### TEST EVALUATION\n",
        "\n",
        "  # find purchased items \n",
        "  test_interacted_items = test_df.groupby('customer_id_idx')['article_id_idx'].apply(list).reset_index()\n",
        "  # combined preds and actual purchases\n",
        "  get_preds_df = pd.merge(test_interacted_items, top_rel_indices_df, how = 'left', left_on = 'customer_id_idx', right_on = ['customer_id'])\n",
        "  # evaluate predictions\n",
        "  get_preds_df['correct_preds'] = [list(set(a).intersection(b)) for a, b in zip(get_preds_df.article_id_idx, get_preds_df.top_rel_articles_preds)]\n",
        "  get_preds_df['recall'] = get_preds_df.apply(lambda x: len(x['correct_preds'])/len(x['article_id_idx']), axis = 1)\n",
        "  get_preds_df['acc'] = get_preds_df.apply(lambda x: len(x['correct_preds'])/len(x), axis=1)\n",
        "\n",
        "  return get_preds_df.acc.mean(), get_preds_df.recall.mean(), get_preds_df.correct_preds, get_preds_df.top_rel_articles_preds"
      ],
      "metadata": {
        "id": "jNW3AjReOaAN"
      },
      "id": "jNW3AjReOaAN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 64\n",
        "n_layers = 2 \n",
        "\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 1024\n",
        "DECAY = 0.001\n",
        "LR = 0.005 \n",
        "K = 12 # HM challenge allows 12 article recommendations"
      ],
      "metadata": {
        "id": "06CEe1FbL64Q"
      },
      "id": "06CEe1FbL64Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval(model, optimizer, train_df):\n",
        "\n",
        "  recall_list = []\n",
        "  acc_list = []\n",
        "  loss_list=[]\n",
        "\n",
        "  for epoch in tqdm(range(EPOCHS)):\n",
        "      # calculate number of batches\n",
        "      n_batch = int(len(train)/BATCH_SIZE)\n",
        "    \n",
        "      # train model\n",
        "      model.train()\n",
        "      for batch_idx in range(n_batch):\n",
        "\n",
        "          # zero out the gradient \n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # data loader to find customer and article purchases\n",
        "          customers, pos_items, neg_items = data_loader(train_df, BATCH_SIZE, n_customers, n_articles)\n",
        "          # get embeddings for each node based on purchase interations\n",
        "          customers_emb, pos_emb, neg_emb, customerEmb0,  posEmb0, negEmb0 = model.encode_minibatch(customers, pos_items, neg_items, train_edge_index)\n",
        "\n",
        "          # compute loss\n",
        "          loss = compute_loss(customers_emb, pos_emb, neg_emb)\n",
        "          # backpropagation\n",
        "          loss.backward()\n",
        "          # take step for gradient\n",
        "          optimizer.step()\n",
        "          # collect loss\n",
        "          loss_list.append(loss)\n",
        "\n",
        "      model.eval() # use trained model\n",
        "      with torch.no_grad(): # zero out gradient\n",
        "          _, out = model(train_edge_index)\n",
        "          final_customer_Embed, final_article_Embed = torch.split(out, (n_customers, n_articles))\n",
        "          accuracy, recall, correct_preds, preds12 = get_preds_and_score(final_customer_Embed, final_article_Embed, n_customers, n_articles, train_df, test_df, K)\n",
        "\n",
        "      acc_list.append(round(accuracy,4))\n",
        "      recall_list.append(round(recall,4))\n",
        "\n",
        "  return (acc_list, recall_list, correct_preds, preds12, loss_list)"
      ],
      "metadata": {
        "id": "bABSoQlvL72I"
      },
      "id": "bABSoQlvL72I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngcf = RecModel(\n",
        "  latent_dim=latent_dim, \n",
        "  num_layers=n_layers,\n",
        "  num_customers=n_customers,\n",
        "  num_articles=n_articles\n",
        ")\n",
        "ngcf.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(ngcf.parameters(), lr=LR)\n",
        "print(\"Size of Learnable Embedding : \", [x.shape for x in list(ngcf.parameters())])"
      ],
      "metadata": {
        "id": "ZXzL_m4GL9zn"
      },
      "id": "ZXzL_m4GL9zn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, recall, correct_preds, preds, train_loss = train_and_eval(ngcf, optimizer, train_df)"
      ],
      "metadata": {
        "id": "G0OjtyItMERC"
      },
      "id": "G0OjtyItMERC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_list = [(i+1) for i in range(EPOCHS*13)]"
      ],
      "metadata": {
        "id": "ORK8koaS9k0Z"
      },
      "id": "ORK8koaS9k0Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_l = []\n",
        "for i in train_loss:\n",
        "  train_l.append(i.cpu().detach().numpy())\n",
        "train_l=np.squeeze(train_l)\n",
        "\n",
        "plt.plot(epoch_list, train_l, label='Training Loss')\n",
        "\n",
        "plt.xlabel('Batches')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "N7VF85lLPD1W"
      },
      "id": "N7VF85lLPD1W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_list = [(i+1) for i in range(EPOCHS)]"
      ],
      "metadata": {
        "id": "dqD21Ul496cy"
      },
      "id": "dqD21Ul496cy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_list, recall, label='Recall')\n",
        "plt.plot(epoch_list, accuracy, label='Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metrics')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "_DOlSGRLPZeQ"
      },
      "id": "_DOlSGRLPZeQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "id": "jyxdOsyX2n9t"
      },
      "id": "jyxdOsyX2n9t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_preds"
      ],
      "metadata": {
        "id": "Fxs4IajH2sOF"
      },
      "id": "Fxs4IajH2sOF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for i in correct_preds:\n",
        "  counter += len(i)\n",
        "\n",
        "# final accuracy\n",
        "counter/len(correct_preds)"
      ],
      "metadata": {
        "id": "WO0aCTqj4o_e"
      },
      "id": "WO0aCTqj4o_e",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "GNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
